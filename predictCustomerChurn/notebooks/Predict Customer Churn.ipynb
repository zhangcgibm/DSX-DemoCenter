{
    "cells": [
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "import wget\n# Customer Information\nwget.download('https://raw.githubusercontent.com/nwngeek212/DSX-DemoCenter/master/predictCustomerChurn/data_assets/customer.csv')\ncustomer = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('customer.csv')\n  \n#Churn information  \nwget.download('https://raw.githubusercontent.com/nwngeek212/DSX-DemoCenter/master/predictCustomerChurn/data_assets/churn.csv')\ncustomer_churn = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('churn.csv')", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "merged=customer.join(customer_churn,customer['ID']==customer_churn['ID']).select(customer['*'],customer_churn['CHURN'])\nmerged.toPandas().head()", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "merged = merged.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\nmerged.toPandas().head()", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "import brunel\nMerged = merged.toPandas()\n%brunel data('Merged') bar x(CHURN) y(EstIncome) mean(EstIncome) color(LocalBilltype) stack tooltip(EstIncome) | x(LongDistance) y(Usage)  tooltip(LongDistance, Usage) :: width=1100, height=400", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "from pixiedust.display import *\ndisplay(merged)", 
            "outputs": [], 
            "metadata": {
                "collapsed": true, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "# Spark SQL also allow you to use standard SQL\nmerged.createOrReplaceTempView(\"merged\")\nsql = \"\"\"\nSELECT c.*\nFROM merged c\nWHERE c.EstIncome>90000\n\n\"\"\"\nspark.sql(sql).toPandas().head()", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "from pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Prepare string variables so that they can be used by the decision tree algorithm\nstringIndexer1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nstringIndexer2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nstringIndexer3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nstringIndexer4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nstringIndexer5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nstringIndexer6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\nstringIndexer7 = StringIndexer(inputCol='CHURN', outputCol='label')\n\n# Pipelines API requires that input variables are passed in  a vector\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")\n\n\n# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n#pipeline = Pipeline(stages=[stringIndexer1, stringIndexer2, stringIndexer3, assembler, rf])\npipeline = Pipeline(stages=[stringIndexer1,stringIndexer2,stringIndexer3,stringIndexer4,stringIndexer5,stringIndexer6,stringIndexer7, assembler, rf])", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "# Split data into train and test datasets\n(trainingData, testingData) = merged.randomSplit([0.7, 0.3],seed=9)\ntrainingData.cache()\ntestingData.cache()", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "# Build model\nmodel = pipeline.fit(trainingData)", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "result=model.transform(testingData)\nresult_display=result.select(result[\"ID\"],result[\"CHURN\"],result[\"Label\"],result[\"prediction\"],result[\"probability\"])\nresult_display.toPandas().head(6)", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "print 'Model Accuracy = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(result))", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "# set different levels for the maxDepth\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder().addGrid(rf.maxDepth,[4,6,8]).build())", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "# perform 3 fold cross validation\ncv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "# train the model\ncvModel = cv.fit(trainingData)\n\n# pick the best model\nbest_rfModel = cvModel.bestModel\n\n# score the test data set\ncvresult=best_rfModel.transform(testingData)", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "print 'Model Accuracy of the best fitted model = {:.2f}.'.format(cvresult.filter(cvresult.label == cvresult.prediction).count()/ float(cvresult.count()))\nprint 'Model Accuracy of the default model = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))\nprint '   '\nprint('Area under the ROC curve of best fitted model = {:.2f}.'.format(evaluator.evaluate(cvresult)))\nprint 'Area under the ROC curve of the default model = {:.2f}.'.format(evaluator.evaluate(result))", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "# Overwrite any existing saved model in the specified path\nbest_rfModel.write().overwrite().save(\"PredictChurn.churnModel\")", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "source": "", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20"
        }, 
        "language_info": {
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python"
        }
    }
}